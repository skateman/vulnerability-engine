"""Platform mock upload module."""

import asyncio
import base64
import hashlib
import json
import os
import signal
import uuid
from datetime import datetime
from datetime import timezone

import uvicorn
from connexion import AsyncApp
from data.rules import RULES
from insights import make_metadata
from insights import rule as insights_rule
from insights import run
from insights.parsers.dnf_modules import DnfModules
from insights.parsers.installed_rpms import Installed
from insights.parsers.yum_repos_d import YumReposD
from insights.specs import Specs
from starlette.endpoints import HTTPEndpoint
from starlette.requests import Request
from starlette.responses import FileResponse
from starlette.responses import JSONResponse
from starlette.responses import Response

from common import mqueue
from common.config import Config
from common.database_handler import DatabasePool
from common.database_handler import DatabasePoolConnection
from common.logging import get_logger
from common.logging import init_logging
from common.strtobool import strtobool

LOGGER = get_logger(__name__)
STORAGE_PATH = "/tmp/storage"
LOOP = asyncio.new_event_loop()

CFG = Config()


@insights_rule(optional=[Specs.machine_id, Installed, YumReposD, DnfModules])
def system_profile(machine_id, installed_rpms, yum_repos_d, dnf_modules):
    profile = {}
    if machine_id:
        profile["id"] = machine_id.content[0].strip()
    profile["installed_packages"] = []
    if installed_rpms:
        for pkg_name in installed_rpms.packages:
            pkg = installed_rpms.get_max(pkg_name)
            profile["installed_packages"].append(pkg.nevra)

    profile["yum_repos"] = []
    if yum_repos_d:
        for repo_file in yum_repos_d:
            if repo_file.file_name == "redhat.repo":
                for repo in repo_file:
                    if repo_file[repo].get("enabled", "1").lower() in ("1", "true", "enabled", "yes", "on"):
                        profile["yum_repos"].append(repo)
                break

    profile["dnf_modules"] = []
    if dnf_modules:
        for module in dnf_modules:
            for module_name in module.sections():
                profile["dnf_modules"].append({"name": module_name, "stream": module.get(module_name, "stream")})
    metadata_response = make_metadata()
    metadata_response.update(profile)
    return metadata_response


def generate_hits(sha1):
    passed = []
    reports = []
    for rule in RULES:
        keys = list(RULES[rule].keys())
        key = keys[int(sha1, 16) % len(keys)]
        if key == "pass":
            passed.append(RULES[rule][key])
        else:
            reports.append(RULES[rule][key])
    return passed, reports


class Application:
    """Base class holding app caches and queues.

    Uses `application` object (lambda func) so the rest of the code does not need to change
    and handlers can use e.g. `self.application.inventory_queue` as before.
    """

    application = lambda: None  # noqa
    application.inventory_queue = mqueue.MQWriter(CFG.events_topic, bootstrap_servers="localhost:9092", loop=LOOP)
    application.advisor_queue = mqueue.MQWriter(CFG.advisor_results_topic, bootstrap_servers="localhost:9092", loop=LOOP)
    application.archive_to_profile_cache = {}
    application.inventory_id_to_profile_cache = {}


class BaseHandler(HTTPEndpoint, Application):
    """Base Handler."""

    def _get_rh_account(self, request: Request):
        if "x-rh-identity" not in request.headers:
            return "Unknown", "0", "0"
        encoded_value = request.headers["x-rh-identity"]
        decoded_value = base64.b64decode(encoded_value).decode("utf-8")
        identity = json.loads(decoded_value)
        identity_type = identity.get("identity", {}).get("type", "Unknown")
        account_number = identity.get("identity", {}).get("account_number", "0")
        org_id = identity.get("identity", {}).get("org_id", "0")
        return identity_type, account_number, org_id


class UploadHandler(BaseHandler):
    """Upload Handler."""

    def _get_upload_multiplier(self, request: Request):
        if "x-upload-multiplier" not in request.headers:
            return 1
        multiplier = request.headers["x-upload-multiplier"]
        return int(multiplier) if multiplier.isdigit() else 1

    def _should_include_rules(self, request: Request):
        return strtobool(request.headers.get("x-include-rules", "false"))

    def _get_reporter(self, request: Request):
        return request.headers.get("x-reporter", "puptoo")

    async def post(self, request: Request):
        """Answer POST request.
        curl -X POST -F "file=@./file.tar.gz" http://localhost:8100/api/v1/upload
        curl -X POST -F "file=@./file.tar.gz" -H "x-upload-multiplier: 10" http://localhost:8100/api/v1/upload
        curl -X POST -F "file=@./file.tar.gz" -H "x-include-rules: true" http://localhost:8100/api/v1/upload
        curl -X POST -F "file=@./file.tar.gz" -H "x-reporter: rhsm-system-profile-bridge" http://localhost:8100/api/v1/upload
        """
        async with request.form() as form:
            if not form or "file" not in form:
                return Response(status_code=400)

            contents = await form["file"].read()
            sha1 = hashlib.sha1(contents).hexdigest()
            file_name = "%s.tar.gz" % sha1
            file_path = os.path.join(STORAGE_PATH, file_name)
            if not os.path.exists(file_path):
                with open(file_path, "wb") as open_file:
                    open_file.write(contents)
            if sha1 in self.application.archive_to_profile_cache:
                profile = self.application.archive_to_profile_cache[sha1]
            else:
                broker = run(system_profile, root=file_path)
                profile = broker[system_profile]
                self.application.archive_to_profile_cache[sha1] = profile
                self.application.inventory_id_to_profile_cache[profile.get("id", None)] = profile
            download_url = "http://platform_mock:8000/api/v1/download/%s" % file_name
            _, rh_account, org_id = self._get_rh_account(request)
            timestamp = datetime.now(timezone.utc).isoformat()
            upload_message = {
                "host": {
                    "id": profile.get("id", None),
                    "account": rh_account,
                    "org_id": org_id,
                    "display_name": sha1 + ".example.com",
                    "system_profile": {
                        "installed_packages": profile["installed_packages"],
                        "yum_repos": [{"id": r, "name": r, "enabled": True} for r in profile["yum_repos"]],
                        "dnf_modules": profile["dnf_modules"],
                        "insights_client_version": "3.0.13-1",
                        "operating_system": {"name": "RHEL", "major": 8, "minor": 0},
                    },
                    "reporter": self._get_reporter(),
                },
                "platform_metadata": {
                    "request_id": str(uuid.uuid1()),
                    "url": download_url,
                    "b64_identity": request.headers.get("x-rh-identity", ""),
                },
                "timestamp": timestamp,
                "type": "created",
            }
            passed, reports = generate_hits(sha1)
            advisor_message = {
                "results": {
                    "pass": passed,
                    "reports": reports,
                },
                "input": {
                    "host": {
                        "id": profile.get("id", None),
                        "account": rh_account,
                        "org_id": org_id,
                        "display_name": sha1 + ".example.com",
                        "reporter": self._get_reporter(),
                    },
                    "platform_metadata": {
                        "request_id": str(uuid.uuid1()),
                        "url": download_url,
                        "b64_identity": request.headers.get("x-rh-identity", ""),
                    },
                    "timestamp": timestamp,
                },
            }
            with DatabasePoolConnection() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        insert into
                        inventory.hosts_v1_1 values (%s, 0, %s, '[]', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, '2030-07-21 05:35:53.682554+00', '{}', %s, '[]')
                        on conflict (id) do update set display_name = EXCLUDED.display_name, updated = CURRENT_TIMESTAMP
                        """,
                        (upload_message["host"]["id"], upload_message["host"]["display_name"], upload_message["host"]["id"]),
                    )
                conn.commit()
            for _ in range(self._get_upload_multiplier()):
                self.application.inventory_queue.send(upload_message)
                LOGGER.info("New inventory upload: %s", upload_message)
                if self._should_include_rules():
                    self.application.advisor_queue.send(advisor_message)
                    LOGGER.info("New advisor upload: %s", advisor_message)
        return Response()


class DownloadHandler(BaseHandler):
    """Download Handler."""

    async def get(self, request: Request):
        """Answer GET request."""
        path = request.path_params["path"]
        file_path = os.path.join(STORAGE_PATH, path)
        if not os.path.isfile(file_path):
            return Response(status_code=404)
        else:
            return FileResponse(file_path, headers={"Content-Type": "application/octet-stream"})


class DeleteHandler(BaseHandler):
    """Delete Handler."""

    async def delete(self, request: Request):
        """Answer DELETE request."""
        _, rh_account, org_id = self._get_rh_account(request)
        inventory_id = request.path_params["inventory_id"]
        delete_message = {"type": "delete", "id": inventory_id, "account": rh_account, "org_id": org_id}
        self.application.inventory_queue.send(delete_message)
        LOGGER.info("Delete: %s", delete_message)
        return Response()


class RbacHandler(BaseHandler):
    """RBAC mock"""

    async def get(self, request):
        """Answer GET request"""
        identity_type, _, _ = self._get_rh_account(request)
        if identity_type in ["User", "ServiceAccount"]:
            return JSONResponse(
                {
                    "meta": {"count": 1, "limit": 10, "offset": 0},
                    "links": {
                        "first": "/api/rbac/v1/access/?application=vulnerability&limit=1000&offset=0",
                        "next": None,
                        "previous": None,
                        "last": "/api/rbac/v1/access/?application=vulnerability,inventory&limit=1000&offset=0",
                    },
                    "data": [
                        {"permission": "vulnerability:*:*", "resourceDefinitions": []},
                        {"permission": "remediations:remediation:read", "resourceDefinitions": []},
                        {"permission": "inventory:hosts:*", "resourceDefinitions": []},
                    ],
                }
            )
        else:
            return Response(status_code=403)


class InsightsRulesHandler(BaseHandler):
    """Insights rule API mock"""

    async def get(self, _):
        """Answer GET request"""
        resp = """{
            "rule_id": "CVE_2017_14491_dnsmasq|CVE_2017_14491_ERROR",
            "created_at": "2019-02-07T19:02:34.162678Z",
            "updated_at": "2020-02-21T13:39:30.779402Z",
            "ruleset": {
                "created_at": "2018-12-21T01:33:00Z",
                "updated_at": "2018-12-21T01:33:00Z",
                "rule_source": "https://gitlab.cee.redhat.com/insights-open-source/insights-security",
                "description": "Security"
            },
            "description": "CVE-2017-14491: dnsmasq code execution with listening processes",
            "active": true,
            "category": {
                "id": 2,
                "name": "Security"
            },
            "impact": {
                "name": "Remote Code Execution",
                "impact": 4
            },
            "likelihood": 4,
            "node_id": "3199382",
            "tags": "cve security",
            "playbook_count": 1,
            "reboot_required": false,
            "publish_date": "2017-10-02T13:00:00Z",
            "summary": "A buffer overflow vulnerability was found in `Dnsmasq`, a popular lightweight DNS and DHCP server. This can lead to remote code execution. Dnsmasq is used either standalone or directly by applications including `libvirt`.\n",
            "generic": "A vulnerability was discovered in Dnsmasq which allows an attacker to overflow a heap buffer and crash or take control of Dnsmasq. This is accomplished through DNS requests to Dnsmasq querying a domain controlled by the attacker. \n\nDnsmasq is a popular lightweight DNS and DHCP server, often used in home networks and cloud environments as a caching DNS\nstub resolver and to manage DHCP leases. It is used either standalone or directly by applications including libvirt, and\nin a number of layered products. libvirt is a management tool for managing Linux containers and virtual machines.\n\nThis vulnerability can lead to remote code execution and could be triggered by a malicious user on the network.\n\nRed Hat recommends that you update Dnsmasq packages to include the [CVE-2017-14491](http://access.redhat.com/security/cve/CVE-2017-14491) security release.",
            "reason": "This system is vulnerable because:\n\n* It is running a vulnerable package",
            "more_info": "* For more information about this specific flaw, see its [knowledge base article](https://access.redhat.com/security/vulnerabilities/3199382).\n*",
            "impacted_systems_count": 0,
            "reports_shown": false,
            "resolution_set": [
                {
                "system_type": 105,
                "resolution": "Red Hat recommends that you update the `dnsmasq` package",
                "resolution_risk": {
                    "name": "Update Package",
                    "risk": 1
                },
                "has_playbook": true
                }
            ],
            "total_risk": 4,
            "hosts_acked_count": 0,
            "rating": 0
            }""".replace(  # noqa: E501
            "\n", ""
        )
        return JSONResponse(resp)


class InventoryHandler(BaseHandler):
    """Inventory mock"""

    async def get(self, inventory_id):
        """Answer GET request"""
        if inventory_id not in self.application.inventory_id_to_profile_cache:
            return Response(status_code=404)
        else:
            prof = self.application.inventory_id_to_profile_cache[inventory_id]
            return JSONResponse(
                {
                    "results": [
                        {
                            "id": inventory_id,
                            "system_profile": {
                                "installed_packages": prof["installed_packages"],
                                "yum_repos": [{"id": r, "name": r, "enabled": True} for r in prof["yum_repos"]],
                                "dnf_modules": prof["dnf_modules"],
                            },
                        }
                    ]
                },
                headers={"Content-Type": "application/json"},
            )


class ExploitHandler(BaseHandler):
    """/api/v1/exploits mock"""

    async def get(self, _):
        """Answer GET request"""
        resp = {
            "content": base64.b64encode(
                json.dumps(
                    {
                        "CVE-2016-1": [
                            {"date": "2022-01-01", "source": "CISA", "reference": "N/A"},
                        ],
                        "CVE-2013-1": [
                            {"date": "2022-01-01", "source": "CISA", "reference": "N/A"},
                        ],
                    }
                ).encode("utf-8")
            ).decode()
        }
        return JSONResponse(resp)


class ServerApplication:
    """Platform mock application."""

    def __init__(self):
        self.app = AsyncApp(__name__)

        self.app.add_url_rule("/api/v1/upload", "upload", UploadHandler, methods=["POST"]),
        self.app.add_url_rule("/api/v1/download/{path}", "download/{path}", DownloadHandler),
        self.app.add_url_rule("/api/v1/delete/{inventory_id}", "delete/{inventory_id}", DeleteHandler, methods=["DELETE"]),
        self.app.add_url_rule("/api/rbac/v1/access", "access", RbacHandler),
        self.app.add_url_rule("/api/inventory/v1/hosts/{inventory_id}/system_profile", "system_profile", InventoryHandler),
        self.app.add_url_rule("/api/insights/v1/rule/{rule_id}", "rule/{rule_id}", InsightsRulesHandler),
        self.app.add_url_rule("/api/v1/exploits", "exploits", ExploitHandler),

        config = uvicorn.Config("platform_mock.platform_mock:app.app", host="0.0.0.0", port=8000, log_level="info")
        self.server = uvicorn.Server(config)


app = ServerApplication()


def main():
    """Main platform mock entrypoint."""
    init_logging()
    if not os.path.exists(STORAGE_PATH):
        os.makedirs(STORAGE_PATH)
    DatabasePool(1)
    LOGGER.info("Starting platform mock.")

    def terminate(*_):
        """Trigger shutdown."""
        LOGGER.info("Signal received, stopping application.")
        LOOP.create_task(app.server.shutdown())

    signals = (signal.SIGHUP, signal.SIGTERM, signal.SIGINT)
    for sig in signals:
        signal.signal(sig, terminate)

    LOOP.run_until_complete(app.server.serve())
    LOGGER.info("Shutting down.")


if __name__ == "__main__":
    main()
